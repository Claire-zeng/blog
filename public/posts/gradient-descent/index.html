<!doctype html><html lang=zh-cn>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<title>梯度下降算法详解 - Claire的技术学习笔记</title>
<script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}}</script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<link href=https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css rel=stylesheet>
<script src=https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js></script>
<link rel=stylesheet href=https://claire-zeng.github.io/blog/css/style.css>
<link rel=icon type=image/x-icon href=https://claire-zeng.github.io/blog/favicon.ico>
</head>
<body>
<header class=site-header>
<div class=container>
<nav class=navbar>
<div class=navbar-brand>
<a href=https://claire-zeng.github.io/blog/ class=navbar-item>
<h1 class=title>Claire的技术学习笔记</h1>
</a>
</div>
<div class=navbar-menu>
<a href=https://claire-zeng.github.io/blog/ class=navbar-item>首页</a>
<a href=https://claire-zeng.github.io/blog/categories class=navbar-item>分类</a>
<a href=https://claire-zeng.github.io/blog/tags class=navbar-item>标签</a>
<a href=https://claire-zeng.github.io/blog/about class=navbar-item>关于</a>
</div>
</nav>
</div>
</header>
<main class=site-main>
<div class=container>
<article class=post>
<header class=post-header>
<h1 class=post-title>梯度下降算法详解</h1>
<div class=post-meta>
<time datetime=2024-01-15>2024年1月15日</time>
<span class=category>数值优化</span>
</div>
<div class=post-tags>
<span class=tag>梯度下降</span>
<span class=tag>机器学习</span>
<span class=tag>一阶优化</span>
<span class=tag>收敛性分析</span>
</div>
</header>
<div class=post-content>
<h1 id=梯度下降算法详解>梯度下降算法详解</h1>
<p>梯度下降是数值优化中最基础和重要的算法之一。本文将详细介绍梯度下降的原理、实现和应用。</p>
<h2 id=算法原理>算法原理</h2>
<p>梯度下降的核心思想是沿着目标函数的负梯度方向进行搜索，以找到函数的局部最小值。</p>
<p>对于目标函数 $f(x)$，梯度下降的更新公式为：</p>
<p>$$x_{k+1} = x_k - \alpha \nabla f(x_k)$$</p>
<p>其中：</p>
<ul>
<li>$x_k$ 是第 $k$ 次迭代的参数值</li>
<li>$\alpha$ 是学习率</li>
<li>$\nabla f(x_k)$ 是函数在 $x_k$ 处的梯度</li>
</ul>
<h2 id=代码实现>代码实现</h2>
<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
<span class=kn>import</span> <span class=nn>matplotlib.pyplot</span> <span class=k>as</span> <span class=nn>plt</span>

<span class=k>def</span> <span class=nf>gradient_descent</span><span class=p>(</span><span class=n>f</span><span class=p>,</span> <span class=n>grad_f</span><span class=p>,</span> <span class=n>x0</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.01</span><span class=p>,</span> <span class=n>max_iter</span><span class=o>=</span><span class=mi>1000</span><span class=p>,</span> <span class=n>tol</span><span class=o>=</span><span class=mf>1e-6</span><span class=p>):</span>
    <span class=s2>&#34;&#34;&#34;
</span><span class=s2>    梯度下降算法实现
</span><span class=s2>    
</span><span class=s2>    参数:
</span><span class=s2>    f: 目标函数
</span><span class=s2>    grad_f: 梯度函数
</span><span class=s2>    x0: 初始点
</span><span class=s2>    alpha: 学习率
</span><span class=s2>    max_iter: 最大迭代次数
</span><span class=s2>    tol: 收敛容差
</span><span class=s2>    
</span><span class=s2>    返回:
</span><span class=s2>    x: 最优解
</span><span class=s2>    history: 迭代历史
</span><span class=s2>    &#34;&#34;&#34;</span>
    <span class=n>x</span> <span class=o>=</span> <span class=n>x0</span>
    <span class=n>history</span> <span class=o>=</span> <span class=p>[</span><span class=n>x</span><span class=o>.</span><span class=n>copy</span><span class=p>()]</span>
    
    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>max_iter</span><span class=p>):</span>
        <span class=n>grad</span> <span class=o>=</span> <span class=n>grad_f</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=n>x_new</span> <span class=o>=</span> <span class=n>x</span> <span class=o>-</span> <span class=n>alpha</span> <span class=o>*</span> <span class=n>grad</span>
        
        <span class=k>if</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>x_new</span> <span class=o>-</span> <span class=n>x</span><span class=p>)</span> <span class=o>&lt;</span> <span class=n>tol</span><span class=p>:</span>
            <span class=k>break</span>
            
        <span class=n>x</span> <span class=o>=</span> <span class=n>x_new</span>
        <span class=n>history</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>copy</span><span class=p>())</span>
    
    <span class=k>return</span> <span class=n>x</span><span class=p>,</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>history</span><span class=p>)</span>

<span class=c1># 示例：最小化 f(x) = x^2 + 2x + 1</span>
<span class=k>def</span> <span class=nf>f</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
    <span class=k>return</span> <span class=n>x</span><span class=o>**</span><span class=mi>2</span> <span class=o>+</span> <span class=mi>2</span><span class=o>*</span><span class=n>x</span> <span class=o>+</span> <span class=mi>1</span>

<span class=k>def</span> <span class=nf>grad_f</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
    <span class=k>return</span> <span class=mi>2</span><span class=o>*</span><span class=n>x</span> <span class=o>+</span> <span class=mi>2</span>

<span class=c1># 运行梯度下降</span>
<span class=n>x_opt</span><span class=p>,</span> <span class=n>history</span> <span class=o>=</span> <span class=n>gradient_descent</span><span class=p>(</span><span class=n>f</span><span class=p>,</span> <span class=n>grad_f</span><span class=p>,</span> <span class=n>x0</span><span class=o>=</span><span class=mf>5.0</span><span class=p>,</span> <span class=n>alpha</span><span class=o>=</span><span class=mf>0.1</span><span class=p>)</span>

<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;最优解: x = </span><span class=si>{</span><span class=n>x_opt</span><span class=si>:</span><span class=s2>.6f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;最优值: f(x) = </span><span class=si>{</span><span class=n>f</span><span class=p>(</span><span class=n>x_opt</span><span class=p>)</span><span class=si>:</span><span class=s2>.6f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</code></pre></div><h2 id=收敛性分析>收敛性分析</h2>
<p>梯度下降的收敛性取决于以下几个因素：</p>
<ol>
<li><strong>学习率选择</strong>：学习率过大可能导致发散，过小则收敛缓慢</li>
<li><strong>函数性质</strong>：凸函数保证收敛到全局最优解</li>
<li><strong>初始点选择</strong>：影响收敛速度和最终解的质量</li>
</ol>
<h2 id=变体算法>变体算法</h2>
<h3 id=随机梯度下降-sgd>随机梯度下降 (SGD)</h3>
<p>对于大规模数据集，可以使用随机梯度下降：</p>
<p>$$x_{k+1} = x_k - \alpha \nabla f_i(x_k)$$</p>
<p>其中 $f_i$ 是第 $i$ 个样本的损失函数。</p>
<h3 id=动量法>动量法</h3>
<p>动量法通过引入动量项来加速收敛：</p>
<p>$$v_{k+1} = \beta v_k + \alpha \nabla f(x_k)$$
$$x_{k+1} = x_k - v_{k+1}$$</p>
<p>其中 $\beta$ 是动量系数。</p>
<h2 id=总结>总结</h2>
<p>梯度下降是数值优化的基础算法，理解其原理和实现对于深入学习机器学习算法至关重要。在实际应用中，需要根据具体问题选择合适的变体和参数设置。</p>
</div>
<footer class=post-footer>
<div class=post-navigation>
<a href=/blog/posts/newton-method/ class=next-post>
牛顿法优化算法 →
</a>
</div>
</footer>
</article>
</div>
</main>
<footer class=site-footer>
<div class=container>
<p>&copy; 2025 Claire的技术学习笔记. All rights reserved.</p>
</div>
</footer>
</body>
</html>