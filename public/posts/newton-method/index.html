<!doctype html><html lang=zh-cn>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<title>牛顿法优化算法 - 数值优化学习笔记</title>
<script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}}</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<link href=https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css rel=stylesheet>
<script src=https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js></script>
<link rel=stylesheet href=https://claire-zeng.github.io/css/style.css>
<link rel=icon type=image/x-icon href=https://claire-zeng.github.io/blog/favicon.ico>
</head>
<body>
<header class=site-header>
<div class=container>
<nav class=navbar>
<div class=navbar-brand>
<a href=https://claire-zeng.github.io/blog/ class=navbar-item>
<h1 class=title>数值优化学习笔记</h1>
</a>
</div>
<div class=navbar-menu>
<a href=https://claire-zeng.github.io/blog/ class=navbar-item>首页</a>
<a href=https://claire-zeng.github.io/blog/categories class=navbar-item>分类</a>
<a href=https://claire-zeng.github.io/blog/tags class=navbar-item>标签</a>
<a href=https://claire-zeng.github.io/blog/about class=navbar-item>关于</a>
</div>
</nav>
</div>
</header>
<main class=site-main>
<div class=container>
<article class=post>
<header class=post-header>
<h1 class=post-title>牛顿法优化算法</h1>
<div class=post-meta>
<time datetime=2024-01-16>2024年1月16日</time>
<span class=category>优化算法</span>
</div>
<div class=post-tags>
<span class=tag>牛顿法</span>
<span class=tag>二阶优化</span>
<span class=tag>数值优化</span>
</div>
</header>
<div class=post-content>
<h1 id=牛顿法优化算法>牛顿法优化算法</h1>
<p>牛顿法是一种基于二阶信息的优化算法，相比梯度下降具有更快的收敛速度。</p>
<h2 id=算法原理>算法原理</h2>
<p>牛顿法利用函数的二阶导数信息来构造更好的搜索方向。对于目标函数 $f(x)$，牛顿法的更新公式为：</p>
<p>$$x_{k+1} = x_k - H^{-1}(x_k) \nabla f(x_k)$$</p>
<p>其中：</p>
<ul>
<li>$H(x_k)$ 是函数在 $x_k$ 处的Hessian矩阵</li>
<li>$\nabla f(x_k)$ 是梯度向量</li>
</ul>
<h2 id=优势与特点>优势与特点</h2>
<ol>
<li><strong>二次收敛</strong>：在最优解附近具有二次收敛速度</li>
<li><strong>二阶信息</strong>：利用Hessian矩阵提供更精确的局部信息</li>
<li><strong>自适应步长</strong>：不需要手动设置学习率</li>
</ol>
<h2 id=代码实现>代码实现</h2>
<div class=highlight><div class=chroma>
<table class=lntable><tr><td class=lntd>
<pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span><span class=lnt>53
</span></code></pre></td>
<td class=lntd>
<pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
<span class=kn>from</span> <span class=nn>scipy.linalg</span> <span class=kn>import</span> <span class=n>inv</span>

<span class=k>def</span> <span class=nf>newton_method</span><span class=p>(</span><span class=n>f</span><span class=p>,</span> <span class=n>grad_f</span><span class=p>,</span> <span class=n>hess_f</span><span class=p>,</span> <span class=n>x0</span><span class=p>,</span> <span class=n>max_iter</span><span class=o>=</span><span class=mi>100</span><span class=p>,</span> <span class=n>tol</span><span class=o>=</span><span class=mf>1e-6</span><span class=p>):</span>
    <span class=s2>&#34;&#34;&#34;
</span><span class=s2>    牛顿法实现
</span><span class=s2>    
</span><span class=s2>    参数:
</span><span class=s2>    f: 目标函数
</span><span class=s2>    grad_f: 梯度函数
</span><span class=s2>    hess_f: Hessian函数
</span><span class=s2>    x0: 初始点
</span><span class=s2>    max_iter: 最大迭代次数
</span><span class=s2>    tol: 收敛容差
</span><span class=s2>    &#34;&#34;&#34;</span>
    <span class=n>x</span> <span class=o>=</span> <span class=n>x0</span>
    <span class=n>history</span> <span class=o>=</span> <span class=p>[</span><span class=n>x</span><span class=o>.</span><span class=n>copy</span><span class=p>()]</span>
    
    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>max_iter</span><span class=p>):</span>
        <span class=n>grad</span> <span class=o>=</span> <span class=n>grad_f</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        <span class=n>hess</span> <span class=o>=</span> <span class=n>hess_f</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
        
        <span class=c1># 求解线性方程组 H(x) * d = -grad(x)</span>
        <span class=k>try</span><span class=p>:</span>
            <span class=n>d</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>solve</span><span class=p>(</span><span class=n>hess</span><span class=p>,</span> <span class=o>-</span><span class=n>grad</span><span class=p>)</span>
        <span class=k>except</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>LinAlgError</span><span class=p>:</span>
            <span class=c1># 如果Hessian不可逆，使用伪逆</span>
            <span class=n>d</span> <span class=o>=</span> <span class=o>-</span><span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>pinv</span><span class=p>(</span><span class=n>hess</span><span class=p>)</span> <span class=o>@</span> <span class=n>grad</span>
        
        <span class=n>x_new</span> <span class=o>=</span> <span class=n>x</span> <span class=o>+</span> <span class=n>d</span>
        
        <span class=k>if</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>x_new</span> <span class=o>-</span> <span class=n>x</span><span class=p>)</span> <span class=o>&lt;</span> <span class=n>tol</span><span class=p>:</span>
            <span class=k>break</span>
            
        <span class=n>x</span> <span class=o>=</span> <span class=n>x_new</span>
        <span class=n>history</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>copy</span><span class=p>())</span>
    
    <span class=k>return</span> <span class=n>x</span><span class=p>,</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>(</span><span class=n>history</span><span class=p>)</span>

<span class=c1># 示例：最小化 f(x) = x^2 + 2x + 1</span>
<span class=k>def</span> <span class=nf>f</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
    <span class=k>return</span> <span class=n>x</span><span class=o>**</span><span class=mi>2</span> <span class=o>+</span> <span class=mi>2</span><span class=o>*</span><span class=n>x</span> <span class=o>+</span> <span class=mi>1</span>

<span class=k>def</span> <span class=nf>grad_f</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
    <span class=k>return</span> <span class=mi>2</span><span class=o>*</span><span class=n>x</span> <span class=o>+</span> <span class=mi>2</span>

<span class=k>def</span> <span class=nf>hess_f</span><span class=p>(</span><span class=n>x</span><span class=p>):</span>
    <span class=k>return</span> <span class=n>np</span><span class=o>.</span><span class=n>array</span><span class=p>([[</span><span class=mi>2</span><span class=p>]])</span>  <span class=c1># 二阶导数为常数2</span>

<span class=c1># 运行牛顿法</span>
<span class=n>x_opt</span><span class=p>,</span> <span class=n>history</span> <span class=o>=</span> <span class=n>newton_method</span><span class=p>(</span><span class=n>f</span><span class=p>,</span> <span class=n>grad_f</span><span class=p>,</span> <span class=n>hess_f</span><span class=p>,</span> <span class=n>x0</span><span class=o>=</span><span class=mf>5.0</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;最优解: x = </span><span class=si>{</span><span class=n>x_opt</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=si>:</span><span class=s2>.6f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
<span class=nb>print</span><span class=p>(</span><span class=sa>f</span><span class=s2>&#34;最优值: f(x) = </span><span class=si>{</span><span class=n>f</span><span class=p>(</span><span class=n>x_opt</span><span class=p>[</span><span class=mi>0</span><span class=p>])</span><span class=si>:</span><span class=s2>.6f</span><span class=si>}</span><span class=s2>&#34;</span><span class=p>)</span>
</code></pre></td></tr></table>
</div>
</div><h2 id=局限性>局限性</h2>
<ol>
<li><strong>计算成本</strong>：需要计算和存储Hessian矩阵</li>
<li><strong>存储需求</strong>：对于高维问题，Hessian矩阵存储成本高</li>
<li><strong>数值稳定性</strong>：Hessian矩阵可能不可逆或病态</li>
</ol>
<h2 id=改进方法>改进方法</h2>
<h3 id=拟牛顿法>拟牛顿法</h3>
<p>拟牛顿法通过近似Hessian矩阵来避免直接计算：</p>
<ul>
<li><strong>BFGS算法</strong>：最常用的拟牛顿法</li>
<li><strong>DFP算法</strong>：另一种经典的拟牛顿法</li>
<li><strong>L-BFGS</strong>：适用于大规模问题的内存友好版本</li>
</ul>
<h2 id=总结>总结</h2>
<p>牛顿法在函数性质良好时具有优秀的收敛性能，但在实际应用中需要考虑计算成本和数值稳定性问题。</p>
</div>
<footer class=post-footer>
<div class=post-navigation>
<a href=/blog/posts/gradient-descent/ class=prev-post>
← 梯度下降算法详解
</a>
</div>
</footer>
</article>
</div>
</main>
<footer class=site-footer>
<div class=container>
<p>&copy; 2025 数值优化学习笔记. All rights reserved.</p>
</div>
</footer>
</body>
</html>