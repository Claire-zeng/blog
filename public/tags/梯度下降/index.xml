<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>梯度下降 on 数值优化学习笔记</title><link>https://claire-zeng.github.io/blog/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/</link><description>Recent content in 梯度下降 on 数值优化学习笔记</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Mon, 15 Jan 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://claire-zeng.github.io/blog/tags/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/index.xml" rel="self" type="application/rss+xml"/><item><title>梯度下降算法详解</title><link>https://claire-zeng.github.io/blog/posts/gradient-descent/</link><pubDate>Mon, 15 Jan 2024 00:00:00 +0000</pubDate><guid>https://claire-zeng.github.io/blog/posts/gradient-descent/</guid><description>梯度下降算法详解 梯度下降是数值优化中最基础和重要的算法之一。本文将详细介绍梯度下降的原理、实现和应用。
算法原理 梯度下降的核心思想是沿着目标函数的负梯度方向进行搜索，以找到函数的局部最小值。
对于目标函数 $f(x)$，梯度下降的更新公式为：
$$x_{k+1} = x_k - \alpha \nabla f(x_k)$$
其中：
$x_k$ 是第 $k$ 次迭代的参数值 $\alpha$ 是学习率 $\nabla f(x_k)$ 是函数在 $x_k$ 处的梯度 代码实现 import numpy as np import matplotlib.pyplot as plt def gradient_descent(f, grad_f, x0, alpha=0.01, max_iter=1000, tol=1e-6): &amp;#34;&amp;#34;&amp;#34; 梯度下降算法实现 参数: f: 目标函数 grad_f: 梯度函数 x0: 初始点 alpha: 学习率 max_iter: 最大迭代次数 tol: 收敛容差 返回: x: 最优解 history: 迭代历史 &amp;#34;&amp;#34;&amp;#34; x = x0 history = [x.copy()] for i in range(max_iter): grad = grad_f(x) x_new = x - alpha * grad if np.</description></item></channel></rss>