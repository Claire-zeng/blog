<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>牛顿法 on 数值优化学习笔记</title><link>https://claire-zeng.github.io/blog/tags/%E7%89%9B%E9%A1%BF%E6%B3%95/</link><description>Recent content in 牛顿法 on 数值优化学习笔记</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Tue, 16 Jan 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://claire-zeng.github.io/blog/tags/%E7%89%9B%E9%A1%BF%E6%B3%95/index.xml" rel="self" type="application/rss+xml"/><item><title>牛顿法优化算法</title><link>https://claire-zeng.github.io/blog/posts/newton-method/</link><pubDate>Tue, 16 Jan 2024 00:00:00 +0000</pubDate><guid>https://claire-zeng.github.io/blog/posts/newton-method/</guid><description>牛顿法优化算法 牛顿法是一种基于二阶信息的优化算法，相比梯度下降具有更快的收敛速度。
算法原理 牛顿法利用函数的二阶导数信息来构造更好的搜索方向。对于目标函数 $f(x)$，牛顿法的更新公式为：
$$x_{k+1} = x_k - H^{-1}(x_k) \nabla f(x_k)$$
其中：
$H(x_k)$ 是函数在 $x_k$ 处的Hessian矩阵 $\nabla f(x_k)$ 是梯度向量 优势与特点 二次收敛：在最优解附近具有二次收敛速度 二阶信息：利用Hessian矩阵提供更精确的局部信息 自适应步长：不需要手动设置学习率 代码实现 import numpy as np from scipy.linalg import inv def newton_method(f, grad_f, hess_f, x0, max_iter=100, tol=1e-6): &amp;#34;&amp;#34;&amp;#34; 牛顿法实现 参数: f: 目标函数 grad_f: 梯度函数 hess_f: Hessian函数 x0: 初始点 max_iter: 最大迭代次数 tol: 收敛容差 &amp;#34;&amp;#34;&amp;#34; x = x0 history = [x.copy()] for i in range(max_iter): grad = grad_f(x) hess = hess_f(x) # 求解线性方程组 H(x) * d = -grad(x) try: d = np.</description></item></channel></rss>