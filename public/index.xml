<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Claire的技术学习笔记</title><link>https://claire-zeng.github.io/blog/</link><description>Recent content on Claire的技术学习笔记</description><generator>Hugo -- gohugo.io</generator><language>zh-cn</language><lastBuildDate>Tue, 16 Jan 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://claire-zeng.github.io/blog/index.xml" rel="self" type="application/rss+xml"/><item><title>牛顿法优化算法</title><link>https://claire-zeng.github.io/blog/posts/newton-method/</link><pubDate>Tue, 16 Jan 2024 00:00:00 +0000</pubDate><guid>https://claire-zeng.github.io/blog/posts/newton-method/</guid><description>牛顿法优化算法 牛顿法是一种基于二阶信息的优化算法，相比梯度下降具有更快的收敛速度。
算法原理 牛顿法利用函数的二阶导数信息来构造更好的搜索方向。对于目标函数 $f(x)$，牛顿法的更新公式为：
$$x_{k+1} = x_k - H^{-1}(x_k) \nabla f(x_k)$$
其中：
$H(x_k)$ 是函数在 $x_k$ 处的Hessian矩阵 $\nabla f(x_k)$ 是梯度向量 优势与特点 二次收敛：在最优解附近具有二次收敛速度 二阶信息：利用Hessian矩阵提供更精确的局部信息 自适应步长：不需要手动设置学习率 代码实现 import numpy as np from scipy.linalg import inv def newton_method(f, grad_f, hess_f, x0, max_iter=100, tol=1e-6): &amp;#34;&amp;#34;&amp;#34; 牛顿法实现 参数: f: 目标函数 grad_f: 梯度函数 hess_f: Hessian函数 x0: 初始点 max_iter: 最大迭代次数 tol: 收敛容差 &amp;#34;&amp;#34;&amp;#34; x = x0 history = [x.copy()] for i in range(max_iter): grad = grad_f(x) hess = hess_f(x) # 求解线性方程组 H(x) * d = -grad(x) try: d = np.</description></item><item><title>关于</title><link>https://claire-zeng.github.io/blog/about/</link><pubDate>Mon, 15 Jan 2024 00:00:00 +0000</pubDate><guid>https://claire-zeng.github.io/blog/about/</guid><description>关于这个博客 欢迎来到Claire的技术学习笔记！
博客介绍 这是一个记录技术学习历程的个人博客。我会在这里分享：
🧮 算法与数据结构：经典算法解析、复杂度分析、实现技巧 💻 编程技术：各种编程语言的学习心得和最佳实践 🔬 数值计算：数值优化、科学计算、机器学习算法 🏗️ 系统设计：架构模式、设计思想、技术选型 📊 数据科学：数据分析、可视化、统计方法 🚀 新技术探索：前沿技术调研、工具使用、项目实践 内容分类 📈 数值优化 梯度下降法及其变体 牛顿法与拟牛顿法 约束优化算法 现代优化方法 💡 算法设计 动态规划 贪心算法 分治策略 图算法 🔧 编程实践 Python高效编程 数据结构实现 性能优化技巧 代码设计模式 🤖 机器学习 监督学习算法 无监督学习 深度学习基础 模型优化 🌐 Web开发 前端技术栈 后端架构 数据库设计 API设计 学习目标 通过这个博客，希望能够：
系统学习：建立完整的技术知识体系，从基础到进阶 实践驱动：通过项目实战加深理解，理论结合实际 知识分享：与技术社区交流学习，互相促进成长 持续进步：跟上技术发展趋势，保持学习热情 关于我 我是一名技术学习者和实践者，对算法、编程和新技术充满热情。希望通过写博客的方式记录学习过程，分享技术心得，与更多朋友交流学习。
如果您对博客内容有任何建议或问题，欢迎通过以下方式联系我：</description></item><item><title>梯度下降算法详解</title><link>https://claire-zeng.github.io/blog/posts/gradient-descent/</link><pubDate>Mon, 15 Jan 2024 00:00:00 +0000</pubDate><guid>https://claire-zeng.github.io/blog/posts/gradient-descent/</guid><description>梯度下降算法详解 梯度下降是数值优化中最基础和重要的算法之一。本文将详细介绍梯度下降的原理、实现和应用。
算法原理 梯度下降的核心思想是沿着目标函数的负梯度方向进行搜索，以找到函数的局部最小值。
对于目标函数 $f(x)$，梯度下降的更新公式为：
$$x_{k+1} = x_k - \alpha \nabla f(x_k)$$
其中：
$x_k$ 是第 $k$ 次迭代的参数值 $\alpha$ 是学习率 $\nabla f(x_k)$ 是函数在 $x_k$ 处的梯度 代码实现 import numpy as np import matplotlib.pyplot as plt def gradient_descent(f, grad_f, x0, alpha=0.01, max_iter=1000, tol=1e-6): &amp;#34;&amp;#34;&amp;#34; 梯度下降算法实现 参数: f: 目标函数 grad_f: 梯度函数 x0: 初始点 alpha: 学习率 max_iter: 最大迭代次数 tol: 收敛容差 返回: x: 最优解 history: 迭代历史 &amp;#34;&amp;#34;&amp;#34; x = x0 history = [x.copy()] for i in range(max_iter): grad = grad_f(x) x_new = x - alpha * grad if np.</description></item></channel></rss>