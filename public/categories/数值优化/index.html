<!DOCTYPE html>
<html lang="zh-cn">
<head><script src="/blog/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=blog/livereload" data-no-instant defer></script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>数值优化 - Claire的技术学习笔记</title>
    
    
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true,
                processEnvironments: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
            },
            chtml: {
                linebreaks: { automatic: true }
            }
        };
    </script>
    
    
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    
    
    <link rel="stylesheet" href="http://localhost:1313/blog/css/style.css">
    
    
    <link rel="icon" type="image/x-icon" href="http://localhost:1313/blog/favicon.ico">
</head>
<body>
    <header class="site-header">
        <div class="container">
            <nav class="navbar">
                <div class="navbar-brand">
                    <a href="http://localhost:1313/blog/" class="navbar-item">
                        <h1 class="title">Claire的技术学习笔记</h1>
                    </a>
                </div>
                <div class="navbar-menu">
                    <a href="http://localhost:1313/blog/" class="navbar-item">首页</a>
                    <a href="http://localhost:1313/blog/categories" class="navbar-item">分类</a>
                    <a href="http://localhost:1313/blog/tags" class="navbar-item">标签</a>
                    <a href="http://localhost:1313/blog/about" class="navbar-item">关于</a>
                </div>
            </nav>
        </div>
    </header>

    <main class="site-main">
        <div class="container">
            
<div class="list-page">
    <header class="page-header">
        <h1 class="page-title">
            
                分类：数值优化
            
        </h1>
        
    </header>

    <div class="posts-list">
        
        <article class="post-item">
            <div class="post-meta">
                <time datetime="2025-07-27">2025年7月27日</time>
                
                <span class="category">数值优化</span>
                
            </div>
            <h2 class="post-title">
                <a href="/blog/posts/%E6%97%A0%E7%BA%A6%E6%9D%9F%E4%BC%98%E5%8C%96%E5%9F%BA%E7%A1%80%E4%B8%80/">无约束优化基础(一)</a>
            </h2>
            <p class="post-excerpt"><h1 id="无约束优化基础一">无约束优化基础(一)</h1>
<blockquote>
<p>🎯 <strong>导读</strong>: 本文将从数学角度深入探讨无约束优化的基本概念，包括全局最小值、局部最小值、以及泰勒定理在优化算法中的重要作用。</p></blockquote>
<hr>
<h2 id="基本概念">基本概念</h2>
<p>在无约束优化中，我们最小化一个依赖于真实值变量的目标函数，并且对这些变量没有任何限制。因此，可以做出如下数学定义：</p>
<div align="center">
<p><strong>优化问题的数学表述</strong></p>
<p>$$\min_{x} f(x) \tag{1}$$</p>
</div>
<p>其中：</p>
<ul>
<li>$x \in \mathbb{R}^n$ 是一个由 $n ≥ 1$ 个分量组成的实数向量</li>
<li>$f:\mathbb{R}^n \to \mathbb{R}$ 是一个光滑的函数</li>
</ul>
<blockquote>
<p>⚠️ <strong>现实约束</strong>: 通常情况下，我们无法直接得到整个函数 $f$ 的全局视野。能得到的只有一系列点 $x_0, x_1, x_2, \ldots$ 的函数值以及相应的导数值。</p></blockquote>
<p>幸运的是，优化算法能够在尽量减少计算开销以及内存的情况下，去寻找一个可以信赖的解。</p>
<hr>
<h2 id="什么是解">什么是解？</h2>
<h3 id="全局最小值-vs-局部最小值">全局最小值 vs 局部最小值</h3>
<p>如果我们能够找到函数 $f$ 的全局最小值点，也就是使得函数取得最小值的地方，那是最完美的。</p>
<div align="center">
<p><strong>全局最小值的定义</strong></p>
<p>$$\text{A point } x^* \text{ is a global minimizer if } f(x^*) \leq f(x) \text{ for all } x$$</p>
</div>
<blockquote>
<p>🔍 <strong>理解要点</strong>: 其中 $x$ 在整个 $\mathbb{R}^n$，或者至少在感兴趣的范围内。</p></blockquote>
<p>由于我们对 $f$ 的了解通常只是局部的，因此全局的最小值往往难以找到。并且由于我们的算法并不会访问很多点（这也是算法本身希望的），我们通常对 $f$ 的整体形状没有好的认识，同时我们永远无法确定该函数在某个未被算法采样的区域内是否出现了急剧抖动。</p></p>
            <div class="post-tags">
                
                <span class="tag">学习笔记</span>
                
                <span class="tag">泰勒公式</span>
                
                <span class="tag">优化算法</span>
                
                <span class="tag">数学分析</span>
                
            </div>
        </article>
        
        <article class="post-item">
            <div class="post-meta">
                <time datetime="2024-01-16">2024年1月16日</time>
                
                <span class="category">数值优化</span>
                
            </div>
            <h2 class="post-title">
                <a href="/blog/posts/newton-method/">牛顿法优化算法</a>
            </h2>
            <p class="post-excerpt"><h1 id="牛顿法优化算法">牛顿法优化算法</h1>
<p>牛顿法是一种基于二阶信息的优化算法，相比梯度下降具有更快的收敛速度。</p>
<h2 id="算法原理">算法原理</h2>
<p>牛顿法利用函数的二阶导数信息来构造更好的搜索方向。对于目标函数 $f(x)$，牛顿法的更新公式为：</p>
<p>$$x_{k+1} = x_k - H^{-1}(x_k) \nabla f(x_k)$$</p>
<p>其中：</p>
<ul>
<li>$H(x_k)$ 是函数在 $x_k$ 处的Hessian矩阵</li>
<li>$\nabla f(x_k)$ 是梯度向量</li>
</ul>
<h2 id="优势与特点">优势与特点</h2>
<ol>
<li><strong>二次收敛</strong>：在最优解附近具有二次收敛速度</li>
<li><strong>二阶信息</strong>：利用Hessian矩阵提供更精确的局部信息</li>
<li><strong>自适应步长</strong>：不需要手动设置学习率</li>
</ol>
<h2 id="代码实现">代码实现</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">from</span> <span class="nn">scipy.linalg</span> <span class="kn">import</span> <span class="n">inv</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">newton_method</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">grad_f</span><span class="p">,</span> <span class="n">hess_f</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    牛顿法实现
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    参数:
</span></span></span><span class="line"><span class="cl"><span class="s2">    f: 目标函数
</span></span></span><span class="line"><span class="cl"><span class="s2">    grad_f: 梯度函数
</span></span></span><span class="line"><span class="cl"><span class="s2">    hess_f: Hessian函数
</span></span></span><span class="line"><span class="cl"><span class="s2">    x0: 初始点
</span></span></span><span class="line"><span class="cl"><span class="s2">    max_iter: 最大迭代次数
</span></span></span><span class="line"><span class="cl"><span class="s2">    tol: 收敛容差
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">x0</span>
</span></span><span class="line"><span class="cl">    <span class="n">history</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">copy</span><span class="p">()]</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">grad</span> <span class="o">=</span> <span class="n">grad_f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">hess</span> <span class="o">=</span> <span class="n">hess_f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="c1"># 求解线性方程组 H(x) * d = -grad(x)</span>
</span></span><span class="line"><span class="cl">        <span class="k">try</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="n">d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">hess</span><span class="p">,</span> <span class="o">-</span><span class="n">grad</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="k">except</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">LinAlgError</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="c1"># 如果Hessian不可逆，使用伪逆</span>
</span></span><span class="line"><span class="cl">            <span class="n">d</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">pinv</span><span class="p">(</span><span class="n">hess</span><span class="p">)</span> <span class="o">@</span> <span class="n">grad</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="n">x_new</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">d</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x_new</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">break</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x_new</span>
</span></span><span class="line"><span class="cl">        <span class="n">history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">history</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 示例：最小化 f(x) = x^2 + 2x + 1</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">grad_f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">hess_f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">2</span><span class="p">]])</span>  <span class="c1"># 二阶导数为常数2</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 运行牛顿法</span>
</span></span><span class="line"><span class="cl"><span class="n">x_opt</span><span class="p">,</span> <span class="n">history</span> <span class="o">=</span> <span class="n">newton_method</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">grad_f</span><span class="p">,</span> <span class="n">hess_f</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="mf">5.0</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;最优解: x = </span><span class="si">{</span><span class="n">x_opt</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;最优值: f(x) = </span><span class="si">{</span><span class="n">f</span><span class="p">(</span><span class="n">x_opt</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span></code></pre></div><h2 id="局限性">局限性</h2>
<ol>
<li><strong>计算成本</strong>：需要计算和存储Hessian矩阵</li>
<li><strong>存储需求</strong>：对于高维问题，Hessian矩阵存储成本高</li>
<li><strong>数值稳定性</strong>：Hessian矩阵可能不可逆或病态</li>
</ol>
<h2 id="改进方法">改进方法</h2>
<h3 id="拟牛顿法">拟牛顿法</h3>
<p>拟牛顿法通过近似Hessian矩阵来避免直接计算：</p></p>
            <div class="post-tags">
                
                <span class="tag">牛顿法</span>
                
                <span class="tag">二阶优化</span>
                
                <span class="tag">拟牛顿法</span>
                
                <span class="tag">Hessian矩阵</span>
                
            </div>
        </article>
        
        <article class="post-item">
            <div class="post-meta">
                <time datetime="2024-01-15">2024年1月15日</time>
                
                <span class="category">数值优化</span>
                
            </div>
            <h2 class="post-title">
                <a href="/blog/posts/gradient-descent/">梯度下降算法详解</a>
            </h2>
            <p class="post-excerpt"><h1 id="梯度下降算法详解">梯度下降算法详解</h1>
<p>梯度下降是数值优化中最基础和重要的算法之一。本文将详细介绍梯度下降的原理、实现和应用。</p>
<h2 id="算法原理">算法原理</h2>
<p>梯度下降的核心思想是沿着目标函数的负梯度方向进行搜索，以找到函数的局部最小值。</p>
<p>对于目标函数 $f(x)$，梯度下降的更新公式为：</p>
<p>$$x_{k+1} = x_k - \alpha \nabla f(x_k)$$</p>
<p>其中：</p>
<ul>
<li>$x_k$ 是第 $k$ 次迭代的参数值</li>
<li>$\alpha$ 是学习率</li>
<li>$\nabla f(x_k)$ 是函数在 $x_k$ 处的梯度</li>
</ul>
<h2 id="代码实现">代码实现</h2>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-python" data-lang="python"><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</span></span><span class="line"><span class="cl"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">grad_f</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="s2">&#34;&#34;&#34;
</span></span></span><span class="line"><span class="cl"><span class="s2">    梯度下降算法实现
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    参数:
</span></span></span><span class="line"><span class="cl"><span class="s2">    f: 目标函数
</span></span></span><span class="line"><span class="cl"><span class="s2">    grad_f: 梯度函数
</span></span></span><span class="line"><span class="cl"><span class="s2">    x0: 初始点
</span></span></span><span class="line"><span class="cl"><span class="s2">    alpha: 学习率
</span></span></span><span class="line"><span class="cl"><span class="s2">    max_iter: 最大迭代次数
</span></span></span><span class="line"><span class="cl"><span class="s2">    tol: 收敛容差
</span></span></span><span class="line"><span class="cl"><span class="s2">    
</span></span></span><span class="line"><span class="cl"><span class="s2">    返回:
</span></span></span><span class="line"><span class="cl"><span class="s2">    x: 最优解
</span></span></span><span class="line"><span class="cl"><span class="s2">    history: 迭代历史
</span></span></span><span class="line"><span class="cl"><span class="s2">    &#34;&#34;&#34;</span>
</span></span><span class="line"><span class="cl">    <span class="n">x</span> <span class="o">=</span> <span class="n">x0</span>
</span></span><span class="line"><span class="cl">    <span class="n">history</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">copy</span><span class="p">()]</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iter</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">        <span class="n">grad</span> <span class="o">=</span> <span class="n">grad_f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">        <span class="n">x_new</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">grad</span>
</span></span><span class="line"><span class="cl">        
</span></span><span class="line"><span class="cl">        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x_new</span> <span class="o">-</span> <span class="n">x</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">tol</span><span class="p">:</span>
</span></span><span class="line"><span class="cl">            <span class="k">break</span>
</span></span><span class="line"><span class="cl">            
</span></span><span class="line"><span class="cl">        <span class="n">x</span> <span class="o">=</span> <span class="n">x_new</span>
</span></span><span class="line"><span class="cl">        <span class="n">history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>
</span></span><span class="line"><span class="cl">    
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">history</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 示例：最小化 f(x) = x^2 + 2x + 1</span>
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="mi">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="k">def</span> <span class="nf">grad_f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
</span></span><span class="line"><span class="cl">    <span class="k">return</span> <span class="mi">2</span><span class="o">*</span><span class="n">x</span> <span class="o">+</span> <span class="mi">2</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 运行梯度下降</span>
</span></span><span class="line"><span class="cl"><span class="n">x_opt</span><span class="p">,</span> <span class="n">history</span> <span class="o">=</span> <span class="n">gradient_descent</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">grad_f</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;最优解: x = </span><span class="si">{</span><span class="n">x_opt</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span><span class="line"><span class="cl"><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&#34;最优值: f(x) = </span><span class="si">{</span><span class="n">f</span><span class="p">(</span><span class="n">x_opt</span><span class="p">)</span><span class="si">:</span><span class="s2">.6f</span><span class="si">}</span><span class="s2">&#34;</span><span class="p">)</span>
</span></span></code></pre></div><h2 id="收敛性分析">收敛性分析</h2>
<p>梯度下降的收敛性取决于以下几个因素：</p></p>
            <div class="post-tags">
                
                <span class="tag">梯度下降</span>
                
                <span class="tag">机器学习</span>
                
                <span class="tag">一阶优化</span>
                
                <span class="tag">收敛性分析</span>
                
            </div>
        </article>
        
    </div>
</div>

        </div>
    </main>

    <footer class="site-footer">
        <div class="container">
            <p>&copy; 2025 Claire的技术学习笔记. All rights reserved.</p>
        </div>
    </footer>

    
    <button id="back-to-top" class="back-to-top-btn" title="回到顶部">
        <svg width="24" height="24" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg">
            <path d="M12 4L4 12H8V20H16V12H20L12 4Z" fill="currentColor"/>
        </svg>
    </button>

    
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const backToTopBtn = document.getElementById('back-to-top');
            
            
            window.addEventListener('scroll', function() {
                if (window.pageYOffset > 300) {
                    backToTopBtn.classList.add('show');
                } else {
                    backToTopBtn.classList.remove('show');
                }
            });
            
            
            backToTopBtn.addEventListener('click', function() {
                window.scrollTo({
                    top: 0,
                    behavior: 'smooth'
                });
            });
        });
    </script>
</body>
</html>
