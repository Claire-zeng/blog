<!doctype html><html lang=zh-cn>
<head>
<meta name=generator content="Hugo 0.92.2">
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<title>数值优化学习笔记</title>
<script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']],displayMath:[['$$','$$'],['\\[','\\]']],processEscapes:!0,processEnvironments:!0},options:{skipHtmlTags:['script','noscript','style','textarea','pre']}}</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<link href=https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css rel=stylesheet>
<script src=https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js></script>
<link rel=stylesheet href=https://claire-zeng.github.io/css/style.css>
<link rel=icon type=image/x-icon href=https://claire-zeng.github.io/blog/favicon.ico>
</head>
<body>
<header class=site-header>
<div class=container>
<nav class=navbar>
<div class=navbar-brand>
<a href=https://claire-zeng.github.io/blog/ class=navbar-item>
<h1 class=title>数值优化学习笔记</h1>
</a>
</div>
<div class=navbar-menu>
<a href=https://claire-zeng.github.io/blog/ class=navbar-item>首页</a>
<a href=https://claire-zeng.github.io/blog/categories class=navbar-item>分类</a>
<a href=https://claire-zeng.github.io/blog/tags class=navbar-item>标签</a>
<a href=https://claire-zeng.github.io/blog/about class=navbar-item>关于</a>
</div>
</nav>
</div>
</header>
<main class=site-main>
<div class=container>
<div class=home-page>
<section class=hero>
<div class=hero-content>
<h1 class=hero-title>数值优化学习笔记</h1>
<p class=hero-subtitle>探索数学之美，理解优化算法</p>
</div>
</section>
<section class=recent-posts>
<h2>最新文章</h2>
<div class=posts-grid>
<article class=post-card>
<div class=post-meta>
<time datetime=2024-01-16>2024年1月16日</time>
<span class=category>优化算法</span>
</div>
<h3 class=post-title>
<a href=/blog/posts/newton-method/>牛顿法优化算法</a>
</h3>
<p class=post-excerpt>牛顿法优化算法 牛顿法是一种基于二阶信息的优化算法，相比梯度下降具有更快的收敛速度。
算法原理 牛顿法利用函数的二阶导数信息来构造更好的搜索方向。对于目标函数 $f(x)$，牛顿法的更新公式为：
$$x_{k+1} = x_k - H^{-1}(x_k) \nabla f(x_k)$$
其中：
$H(x_k)$ 是函数在 $x_k$ 处的Hessian矩阵 $\nabla f(x_k)$ 是梯度向量 优势与特点 二次收敛：在最优解附近具有二次收敛速度 二阶信息：利用Hessian矩阵提供更精确的局部信息 自适应步长：不需要手动设置学习率 代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 import numpy as np from scipy.</p>
<div class=post-tags>
<span class=tag>牛顿法</span>
<span class=tag>二阶优化</span>
<span class=tag>数值优化</span>
</div>
</article>
<article class=post-card>
<div class=post-meta>
<time datetime=2024-01-15>2024年1月15日</time>
<span class=category>优化算法</span>
</div>
<h3 class=post-title>
<a href=/blog/posts/gradient-descent/>梯度下降算法详解</a>
</h3>
<p class=post-excerpt>梯度下降算法详解 梯度下降是数值优化中最基础和重要的算法之一。本文将详细介绍梯度下降的原理、实现和应用。
算法原理 梯度下降的核心思想是沿着目标函数的负梯度方向进行搜索，以找到函数的局部最小值。
对于目标函数 $f(x)$，梯度下降的更新公式为：
$$x_{k+1} = x_k - \alpha \nabla f(x_k)$$
其中：
$x_k$ 是第 $k$ 次迭代的参数值 $\alpha$ 是学习率 $\nabla f(x_k)$ 是函数在 $x_k$ 处的梯度 代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 import numpy as np import matplotlib.</p>
<div class=post-tags>
<span class=tag>梯度下降</span>
<span class=tag>机器学习</span>
<span class=tag>数值优化</span>
</div>
</article>
</div>
</section>
</div>
</div>
</main>
<footer class=site-footer>
<div class=container>
<p>&copy; 2025 数值优化学习笔记. All rights reserved.</p>
</div>
</footer>
</body>
</html>